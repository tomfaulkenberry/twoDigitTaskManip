---
title             : "Task instructions modulate unit-decade binding in two-digit number representation"
shorttitle        : "Unit-decade binding in two-digit numbers"

author: 
  - name          : "Thomas J. Faulkenberry"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Psychological Sciences, Box T-0820, Tarleton State University, Stephenville, TX 76401"
    email         : "faulkenberry@tarleton.edu"
  - name          : "Alexander Cruise"
    affiliation   : "2"
  - name          : "Samuel Shaki"
    affiliation   : "2"

affiliation:
  - id:           "1"
    institution:  "Tarleton State University"
  - id:           "2"
    institution:  Ariel University
 
author_note: >
  This paper was written in RMarkdown, with code for data analysis integrated into the manuscript text.  The RMarkdown script and data are open and freely available at https://git.io/vhU7z.  
  
  The authors would like to thank Brie Heidingsfelder, Jonathan Herring, Heather Hill, and Kate Shaw for their assistance with data collection.  We would also like to thank Michael Steinborn and an anonymous reviewer for their helpful comments on an earlier version of this manuscript. 

abstract: >
  Previous studies have found decomposed processes, as well as holistic processes, in the representation of two-digit numbers. The present study investigated the influence of task instruction on such processes. Participants completed both magnitude and parity tasks in one of three instructional conditions, where they were asked to either consider two-digit numbers as a whole or to focus on one specific digit.  In two experiments, we found that when participants were asked to consider the two digits as an integrated number, they always exhibited a unit-decade compatibility effect, indicating a failure of selective attention on the digit relevant to the given task.  However, the mere presence of the neighboring digit is not a sufficient condition for the compatibility effect: when participants were explicitly asked to process a specific digit, their success/failure to selectively ignore the irrelevant digit depended on task requirements. Further, computer mouse tracking indicated that the locus of the compatibility effect was related to late response-related processing. The results signify the deep involvement of top-down processes in unit-decade binding for two-digit number representation. 


keywords          : "Unit-decade compatibility, Selective Attention, Computer mouse tracking, Top-down processing"
wordcount         : " "

bibliography      : ["references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r start, include = FALSE}
library(papaja)
library(tidyverse)
source("~/Dropbox/papers/2018/twoDigitTaskManip/submitted/helperFunctions.R")

```

When a child first learns to associate numerical digits with underlying numerical quantities, the transition from single-digit numbers to two-digit numbers presents a challenge.  Indeed, two-digit numbers may be initially treated in such a fashion that the numbers 19 or 91 may not be discriminated from each other. Later on, however, the child learns about place value, where the specific location of each digit has its own value and even name (nine or ninety). With increasing experience, individuals learn to integrate the separate digits into holistic magnitudes [@klein2013;@mussolin2008]. This leads to a natural question: Are the single digits of a two-digit number indeed merged into an inseparable internal magnitude code (i.e., holistic processing)?  Or is it perhaps the case that people can flexibly form separate magnitude representations of the single-digits that comprise a two-digit number (i.e., decomposed processing)? 

## Models of two-digit number processing

Early work on this question indicated evidence of holistic processing. @hinrichs1981 asked participants to compare presented two-digit numbers to a fixed standard (e.g., 55). Hinrichs et al. found that response times increased as the numerical distance from the standard decreased. This paralleled the classic numerical distance effect previously found with single digit numbers [@moyer1967] and led Hinrichs et al. to conclude that two digit numbers are processed holistically as single, integrated quantities.  Similarly, @dehaene1990 also found that response times increased as numerical distance decreased, concluding also that symbolic two-digit numbers must be first encoded into an internal, analogue representation. 

Evidence favoring decomposed processing was reported in the seminal study by @nuerk2001, who found a unit-decade compatibility effect in comparison of two-digit numbers. When the decade and unit digits of one number were both smaller (or both larger) than both digits of the other number (e.g., 23 versus 45), reaction times (RTs) were faster than if the digits were unit-decade *incompatible* with each other (e.g. 29 versus 51). This finding indicates that participants make parallel comparisons of the decade and unit digits when comparing two-digit numbers, even though the decision can be made entirely by comparing the decade digits alone [see also @moeller2009].

The compatibility between the decade and unit digits can also explain the effect of overall distance that is commonly found in two-digit number comparison tasks [@dehaene1990; @hinrichs1981; @verguts2005]. As mentioned earlier, participants tend to respond faster when deciding whether 68 is larger than 55 than they do when deciding whether 61 is larger than 55, although a simple decade comparison is sufficient for deciding in both cases. Indeed, holistic processing may predict this distance effect, as the overall distance between 68 and 55 is larger than the overall distance between 61 and 55 [@moyer1967]. However, the distance effect observed here may instead be an artifact of unit-decade compatibility. Hence, measuring the distance effect alone seems to be inconclusive regarding the nature of two-digit representations [e.g. @dewolf2014; for an extended discussion, see @nuerk2011].

The unit-decade compatibility effect has been replicated in many studies [@moeller2013;@moeller2009;@nuerk2005]. Recently, some investigations have focused on examining the factors that affect the attentional weight of the decade and unit digits.  For example, the unit digit has been found to be more salient for German participants, who read the unit-digit first (e.g. 24 is said ``four and twenty'') than for English, Spanish, or Italian speakers [@macizo2010;@nuerk2005language].  So, the amount of attention allocated to the two-digit numerals' components may depend on the structure of the number words within a language. Also, @castronovo2011 found that the mode of presentation of the two-digit numbers (e.g., tens first, unit first, or simultaneously) could determine how much attention is paid to each component of the two-digit numbers. In addition, @macizo2011 manipulated the stimulus list by increasing the ratio of within-decade comparisons, which resulted in a larger unit-decade compatibility effect. This indicates that even a simple stimulus manipulation can induce participants to allocate more attention to the unit-digit.

Finally, in order to test if the processing of two-digit numbers depends also on task characteristics, @reynvoet2011 asked participants to perform either magnitude or parity judgments on two-digit targets preceded by masked primes. The briefly-presented primes contained one of the target digits in either a task-congruent (3\# $\rightarrow$ 37) or a task-incongruent (\#3 $\rightarrow$ 37) position. The first digit mainly mediated the priming effects (e.g., faster RTs for task-congruent primes) in a magnitude task, presumably because the decade digit has the largest predictive value when comparing magnitudes [see also @ganorStern2007;@ratinckx2005]. The opposite pattern was found in the parity task, probably due to the importance of the unit digit in such a task.

In summary, most recent studies agree that the magnitude of two-digit numbers is not only represented as one holistic entity, but also decomposed for its components (units and decades). Participants' failure to compare the decade digits only while ignoring irrelevant unit digits (the unit-decade compatibility effect) has served as the key marker in this notion of decomposed processing [@moeller2013]. 

## Effects of task instruction

Note that in these previous studies, participants were asked to consider the two-digit numbers as holistic mathematical units (e.g., to decide if two-digit number is smaller or larger than 55). The unit-decade compatibility effect measured their ability to process the two-digit number as integral mathematical unit (holistically) or not (decomposed processing). In the present study, however, we tackle the compatibility effect from another direction: if participants are asked to process two-digit numbers in a specific decomposed manner, will they be able to exclusively process the relevant digit, while simultaneously ignoring the irrelevant digit?

Two recent studies imply that the mere *presence* of adjacent digits can be enough to elicit automatic processing of the irrelevant digit. @nuerk2005power used an Eriksen flanker task, in which three identical distractor stimuli appeared on both left and right sides of a single-digit target. They found that when distractor stimuli led to the same response as the target, responses were faster than when they were associated with a different response, demonstrating that participants processed the magnitude of the irrelevant adjacent digit. Similarly, @kallai2012 presented single-digit numbers as three digit strings with zeros in two of three positions. They manipulated the position of the nonzero digit, presenting it in either the hundreds position, the decade position, or the unit position.  Even though participants were asked to ignore the zeros, Kallai and Tzelgov found that the position of the nonzero digit had a significant effect on response patterns, even though the position was irrelevant to the task.  Once again, such a result indicates that participants may automatically form representations of the magnitude of multiple digits.

Note that both of the above studies specifically used a magnitude task. In addition, the location of the adjacent digit(s) was not controlled. For example, the distractor stimuli in @nuerk2005power were presented on both the left and right sides of the target. Therefore, these studies do not allow us to fully investigate the influence of specific task characteristics, or the potential interaction between task (i.e., magnitude or parity) and the side (left or right) of the adjacent digit. Nevertheless, these studies imply that at least in *some* cases, (a) the mere presence of digits next to each other in a vertically-aligned fashion is sufficient for participants to process them automatically even without an explicit instruction, and (b) that participants are aware of the place-value of a digit even if it is irrelevant to the task.  

To summarize, systematic studies into the influence of task instructions on unit-decade binding in two-digit numbers remain scarce, and the debate on its specific role is ongoing. In our study, we aimed to explore the effect of *task instructions* on the processing of two-digit numbers by using the unit-decade compatibility effect as a marker of decomposed processing.  Our second aim was to examine the flexibility and boundary conditions under which these binding effects appear. Precisely, we asked whether the presence of two digits next to each other was sufficient for participants to treat them as a two-digit number. Or, perhaps, is it necessary to explicitly define our stimuli as a two-digit number (through task instructions) in order for such unit-decade binding to occur?

Why should specific task instructions matter? Evidence in support of instruction-based modulation of response activation comes from studies showing that specific stimulus-response mappings induced by task instructions can trigger response tendencies in an irrelevant task [e.g., @cohenKdoshay2007; @liefooghe2013;@wenke2005;@wenkeGaschler2005]. Thus, we expect that task instructions might not only serve to associate digits with specific responses in a single task, but also co-activate a response based on the neighboring digit, thus effectively binding the components of two-digit number into a single, unitary representation. This hypothesis is predicted directly by a dual-route model which assumes that task-relevant and task-irrelevant information are processed in parallel by two independent routes [e.g., @deJong1994;@kornblum1990;@zorzi1995]. 

The mechanism for this dual-route model is as follows. One route (a *conditional route*) processes task-relevant stimulus information based on current task instructions, which determine how stimulus events and actions are bound together in given situation.  For example, in a parity judgment task, task instructions will specify that if a number is even, the participant should press the left button, and conversely, if a number is odd, one press the right button.  Thus, the task instruction produces two stimulus-response mappings: "even = right" and "odd = left". At the same time, an *unconditional route* processes task-irrelevant stimulus information (e.g., an irrelevant flanker digit) and primes its associated motor response. According to the model, response duration depends on the degree to which the instruction-induced SR mappings (associated with the two routes) are shared by the two competing stimuli. That is, if the two routes converge on the same response, processing is *facilitated* (i.e., fast RTs), whereas if the two routes activate opposing responses, processing is *inhibited* (i.e., slow RTs). For a concrete example, consider the number 13. If participants are asked to make a parity judgment of the unit digit only (i.e., the "3"), the dual-route model predicts that participants will be relatively quick to make the judgment, as both the conditional route (mapping the stimulus "3" to the response "press left button") and the unconditional route (mapping the irrelevant stimulus "1" to the response "press left button") converge to the same response. However, if the number 23 is presented, we may expect an interference effect, thus slowing down response times.  This is because the conditional route maps the task relevant digit "3" to the response "press left button" (because 3 is odd), but the unconditional route maps the irrelevant digit "2" to the response "press right button" (because 2 is even).  

However, this prediction assumes that participants implicitly place equal weight on the information carried by the two digits.  For example, in a parity judgment, only the unit digit is relevant to the decision (e.g., the even/odd status of 23 is determined *completely* by the unit digit 3). Thus, in the context of a parity task on unit digits, the decade digit is irrelevant to the decision (even in the context of a two-digit number), and thus the activation of the unconditional route may be reduced. As a result, any processing in the unconditional route may not weigh as heavily on the final decision. Thus, the interference between response mappings may be reduced, reflecting a role for top-down control in the dual-route model.  At present, the exact nature of these response conflicts in two-digit number representation is not clear; the need for such descriptive data forms the basis for the present study.

## The present study

In the present study, we sought to understand the conditions under which task instructions can modulate unit-decade binding in two digit numbers. To this end, we asked participants to separately perform magnitude and parity tasks with two-digit numbers. Within each task, we gave three different types of instructions: *overall*, *decade-only*, and *unit-only*. In the overall condition, the participants’ task was to consider the stimuli as a "two-digit number".  Thus, for the magnitude task, the specific instruction was to decide if the two-digit number was smaller or larger than 55.  For the parity task, the specific instruction was to decide if the two-digit number was odd or even.  The overall condition served as a benchmark for our other critical decomposed conditions. In the two decomposed instructional conditions, participants were explicitly instructed to pay attention to only the decade or the unit digit.  In the magnitude task, they were to decide if the single relevant digit was smaller/larger than 5, and in the parity task, they were to decide whether the single digit was odd/even.  Importantly, in these decomposed instructional conditions, the adjacent digit always served as a task-irrelevant stimulus and was unassociated with the correct response required for the target digit. Hence, we were especially interested in observing whether participants could ignore the irrelevant digit and focus their attention exclusively on the target digit.

In our experiments, we explicitly manipulated the *congruency* of the digits in each stimulus.  The definitions of congruent/incongruent depended on task.  For example, in the parity task, trials were congruent when both the task-relevant and task-irrelevant digits required the same response (i.e., both digits odd or both even, such as 37 or 46).  Parity trials were incongruent when the two digits required opposite responses (i.e., one digit odd and the other even, such as 78).  Congruency on the magnitude task was defined similarly; trials were congruent when both digits required the same response (i.e., both digits smaller or larger than 5, such as 23 or 78) and incongruent when digits resulted in opposite responses (i.e., one digit smaller than 5 and the other larger, such as 28 or 73).  In light of this congruency manipulation, we hypothesized that our instructional conditions (overall, decade-only, or unit-only) would modulate attentional focusing towards the irrelevant digit only when the irrelevant digit is relevant for completion of a specific task. As a consequence, in the magnitude task, we predict that greater interference will occur when the distractor is in the decade position (i.e., when participants are asked to pay attention to only the unit digit). In contrast, in the parity task, we expect more pronounced interference from neighbors in the unit position (i.e., when participants are asked to pay attention to only the decade digit).

We tested these predictions in two experiments that used similar stimuli and procedures, but we varied the response mode. In Experiment 1, participants categorized parity/magnitude status of a target digit using left-right keypress responses, from which we measured response times (RTs) and error rates. In Experiment 2, we used computer mouse tracking [@freeman2009motions;@freemanDaleFarmer2011;@spivey2005] to record the kinematics of participants' hand movements as they categorized parity/magnitude by moving a mouse cursor from a central "start" area to a target areas positioned in the upper left or right corner of the screen. Such an approach has been used in several recent studies in numerical cognition [@santens2011;@songNakayama2008;@faulkenberry2014;@faulkenberry2015;@faulkenberry2016;@faulkenberryShaki2016;@faulkenberry2017;@faulkenberry2018;@erb2018]. Importantly, this approach allowed us two empirical advantages.  First, we were able to record the streaming $x,y$-coordinates of the computer mouse movements during the decision process. Any systematic spatial deviations between mouse trajectories that arise from our experimental manipulations would provide evidence of competition between response options that cannot be easily detected from patterns of RTs alone. Second, computer mouse tracking gives us a means to record response times in a decomposed fashion, breaking the total RT into components of *initiation time* (the time elapsed between stimulus presentation and initial mouse movement) and *movement time* (reflecting the actual duration of mouse movement). This provides a potentially sensitive method for further discerning the locus of any RT patterns obtained in Experiment 1, which will allow us to effectively test the generality of our predictions across different response modes.

# Experiment 1

## Method
### Participants
Sixteen students (6 male, 10 female, mean age 22.1 years, SD = 2.03 years) from Ariel University participated in three experimental sessions in exchange for partial course credit.  Two of the male participants reported that they were left handed.  All participants were born in Israel and were Hebrew-English bilinguals. 

### Stimuli and apparatus
The stimulus set consisted of all two-digit numbers in the range 11-99, excluding numbers with 5 or 0 in it (e.g., 40 or 51). These 64 two-digit numbers were replicated, resulting in 128 trials in a single block (see below for design). Half of the stimuli were smaller than 55 and half were larger than 55. Half of the numbers were odd and the other half were even.  Finally, the number of congruent and incongruent numbers was equal in both tasks. The stimuli were presented in their Arabic form and appeared in black Times New Roman font (size 30) at the center of the screen on a white background of a 17-in. (43 cm, 1024 X 768 pixel resolution) monitor. Responses were made on a standard QWERTY keyboard, with all keys covered except A (left hand responses) and L (right hand responses). Response times and accuracy were recorded with SuperLab 2.0.

### Design
All independent variables were manipulated within-subjects. The 128 number stimuli described above were repeated (randomly without replacement) in 12 different blocks, each of which was defined via the factorial manipulation of three independent variables, which we describe as follows. There were three instructional conditions where we manipulated the target (overall, decade, unit) and two different task orders (magnitude task first, parity task first), all of which were performed under two different response rules, depending on task ("small=left" or "small=right" for the magnitude task, and "even=left" or "even=right" for the parity task).  All together, this factorial combination (12 blocks of 128 trials) resulted in `r 12*128` trials in total per participant.

Participants performed these 12 blocks of trials in three separate experimental sessions, each separated by one week. In the first session, participants completed four blocks (`r 4*128` trials) of the "overall" instructional condition, performing both the magnitude and parity tasks under the two different response rules. Then, in each of the two subsequent sessions, participants repeated this design for both the "decade" and "unit" instructional conditions, completing 4 experimental blocks each day (`r 4*128` trials). Note that the two response rules for each task (parity, magnitude) were always done in two consecutive blocks. The task order, the decade and unit conditions order, and the order of the response rules were counterbalanced across participants, but were kept fixed along the experiment for each participant. Finally, the order of stimuli was randomized within each block.

### Procedure
Data were collected in a dimly lit room with the participant seated approximately 50 cm from the center of the screen.  In each trial, a fixation cross appeared for 500 ms on a blank screen and was then followed by a randomly selected two-digit number. In the overall condition, the participant’s task was to decide if the two-digit number is smaller or larger than 55 (magnitude task) or if the two-digit number is odd or even (parity task). In the decade and the unit conditions, the participant's task was to decide if the relevant digit (leftmost digit or rightmost digit) is smaller or larger than five (magnitude task), or if the relevant digit is odd or even (parity task). The left/right response keys served for odd/even and smaller/larger responses, according to the previously agreed response rule.  Each stimulus number stayed on the screen until a response was made, and the next trial began 500 ms later. 

A planned break of 15 minutes between tasks was given, while a short 5 minute break separated the two consecutive blocks of response rules within each task. Each session took approximately 50 minutes to complete, and approximately one week separated between the three different sessions.

## Results

```{r datagrab, cache=TRUE}
##############
#DATA SETUP
##############

# import data
dataRaw <- read_csv("~/Dropbox/papers/2018/twoDigitTaskManip/exp1data/dataExp1.csv")

# fix factor labels and orderings for plots
dataRaw$condition = factor(dataRaw$condition, levels = c("general","decade","unit"), labels = c("Overall", "Decade", "Unit"))

correctTrials = subset(dataRaw, subset=correct==1)
sub = as.factor(correctTrials$subject)
subjectParams = function(dat){
  meanRT = mean(dat$rt)
  sdRT = sd(dat$rt)
  c(meanRT, sdRT)
}

bySubject = matrix(unlist(by(correctTrials, sub, subjectParams)), ncol=2, byrow=TRUE)
    
# alternative cleaning method from Steinborn (reviewer 1)
nSub = length(unique(dataRaw$subject))
outlier = numeric(length(dataRaw$subject))
for (i in 1:length(dataRaw$subject)){
  meanRT = bySubject[dataRaw$subject[i] ,1]
  sdRT = bySubject[dataRaw$subject[i], 2]
  if (dataRaw$rt[i] > meanRT + 2.5*sdRT){
    outlier[i] <- 1
  }
  else {
    outlier[i] <- 0
  }
}

dataRaw$outlier = outlier

# basic info
nTrials = dim(dataRaw)[1]
numOutliers = sum(dataRaw$rt < 200 | dataRaw$outlier==1)


outlierRate = dataRaw %>%
  group_by(subject) %>%
  summarize(outl = 100*sum(outlier)/length(outlier))

dataWithErrs = dataRaw %>%
  filter(rt>200 & outlier==0)

# remove errors for RT analysis
data = dataRaw %>%
    filter(correct==1 & rt>200 & outlier==0)

```

Participants completed a total of `r nTrials +2` experimental trials.  Two trials were inexplicably not recorded in the data file. From the remaining `r nTrials` trials, we initially discarded `r numOutliers` outlier trials using the following procedure. For each factorial combination of participant and experimental condition (i.e., each *design cell*), we computed the mean and standard deviation of the distribution of response times for correct trials. Then, we discarded any trial for which the response time satisfied one of two conditions: (a) the RT was less than 200 ms; or (b) the RT was larger than the mean RT plus 2.5 standard deviations, where mean and standard deviation were calculated for the specific design cell from whence the trial originated.  After this cleaning procedure, we retained `r round(100*(nTrials-numOutliers)/nTrials, 1)`% of the original trials for our data analysis.  Across the `r nSub` participants, the percentage of outliers for each participant ranged between `r round(min(outlierRate$outl),2)`% and `r round(max(outlierRate$outl),2)`% (mean outlier rate = `r round(mean(outlierRate$outl),2)`%).

Unit-decade congruency was defined in a conceptually similar way for both magnitude and parity tasks: a two-digit number was defined as *congruent* if both decade and unit digits led to an identical response. The number was defined as incongruent if the digits yielded conflicting responses. For instance, the two-digit number 34 was considered as congruent in the magnitude task (since both decade and unit are less than 5), but as an incongruent in the parity task (since 3 is odd, but 4 is even).

```{r exp1table, results="asis", include=TRUE}


# format table as "magnitude" and "parity", symmetric with other plots in manuscript

# magnitude sub-table
magRT <- data %>%
  filter(task=="magnitude") %>%
  group_by(condition, congruency) %>%
  summarize(meanRT = mean(rt),
            sdRT = sd(rt)
            )

magErr <- dataWithErrs %>%
  filter(task=="magnitude") %>%
  mutate(err=1-correct) %>%
  group_by(condition, congruency) %>%
  summarize(errRate = 100*sum(err)/length(err))

magTable <- right_join(magRT,magErr)
names(magTable) <- c("Condition", "Congruency", "Mean", "SD", "Error rate (%)")

# parity sub-table
parityRT <- data %>%
  filter(task=="parity") %>%
  group_by(condition, congruency) %>%
  summarize(meanRT = mean(rt),
            sdRT = sd(rt)
            )

parityErr <- dataWithErrs %>%
  filter(task=="parity") %>%
  mutate(err=1-correct) %>%
  group_by(condition, congruency) %>%
  summarize(errRate = 100*sum(err)/length(err))

parityTable <- right_join(parityRT,parityErr)
names(parityTable) <- c("Condition", "Congruency", "Mean", "SD", "Error rate (%)")

apa_table(
  list('Magnitude task' = magTable, 'Parity task' = parityTable),
  caption = "Descriptive statistics for performance measures in Experiment 1",
  note = "Mean RT and SD were computed for correct trials only.",
  col_spanners = list("RT (ms)" = c(3,4)),
  align = c("l", "l", "c", "c", "c"),
  escape=TRUE,
  landscape=FALSE)


```

```{r exp1, results="hide", fig.height=4, fig.width=6, fig.cap="Mean RTs for Experiment 1, presented as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and task (magnitude, parity). Error bars represent within-subject 95% confidence intervals as recommended by @morey2008.", cache=TRUE}

# plot figure 1 in paper
agg=aggregate(rt~subject+condition+task+congruency,data=data,FUN="mean") 

summary=summarySEwithin(agg,measurevar="rt",withinvars=c("congruency","task","condition"),idvar="subject")

summary %>%
  ggplot(aes(x=condition,y=rt,shape=congruency)) +
  geom_line(aes(group=congruency,linetype=congruency))+
  geom_point(size=4)+
  geom_errorbar(width=0.1,aes(ymin=rt-ci,ymax=rt+ci))+
  labs(x="Instruction Condition",y="Mean RT (ms)")+
  facet_grid(.~task)+
  theme_classic(14)

# analyses
detach(package:plyr)
data1 = data %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(condition=="Overall") %>%
  group_by(subject, task, congruency) %>%
  summarize(mRT = mean(rt)) 

model1.aov=aov(mRT ~ task*congruency + Error(subject/(task*congruency)), data=data1)
summary1=apa_print(model1.aov, es="pes", mse=FALSE)

data2 = data %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(condition!="Overall") %>%
  group_by(subject, task, congruency, condition) %>%
  summarize(mRT=mean(rt))

model2.aov = aov(mRT ~ task*congruency*condition + Error(subject/(task*congruency*condition)), data=data2)
summary2=apa_print(model2.aov, es="pes", mse=FALSE)
maxNonSig = max(sort(as.numeric(summary2[["table"]]$`$F$`))[1:3])

# descriptives for main effects
summary2task = data %>%
  filter(condition != "Overall") %>%
  group_by(task) %>%
  summarize(mRT = mean(rt))
taskEffect= summary2task$mRT[2]-summary2task$mRT[1]

summary2cong = data %>%
  filter(condition != "Overall") %>%
  group_by(congruency) %>%
  summarize(mRT = mean(rt))
congEffect = summary2cong$mRT[2]-summary2cong$mRT[1]

summary2condition = data %>%
  filter(condition != "Overall") %>%
  group_by(condition) %>%
  summarize(mRT = mean(rt))
congEffect = summary2cong$mRT[2]-summary2cong$mRT[1]

# t-tests for describing 3-way interaction
magDecade = data %>%
  filter(condition=="Decade" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mRT = mean(rt))
testMagDecade = apa_print(t.test(magDecade$mRT[magDecade$congruency=="incongruent"], magDecade$mRT[magDecade$congruency=="congruent"],paired=TRUE))

parityDecade = data %>%
  filter(condition=="Decade" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mRT = mean(rt))
testParityDecade = apa_print(t.test(parityDecade$mRT[parityDecade$congruency=="incongruent"], parityDecade$mRT[parityDecade$congruency=="congruent"],paired=TRUE))

magUnit = data %>%
  filter(condition=="Unit" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mRT = mean(rt))
testMagUnit = apa_print(t.test(magUnit$mRT[magUnit$congruency=="incongruent"], magUnit$mRT[magUnit$congruency=="congruent"],paired=TRUE))

parityUnit = data %>%
  filter(condition=="Unit" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mRT = mean(rt))
testParityUnit = apa_print(t.test(parityUnit$mRT[parityUnit$congruency=="incongruent"], parityUnit$mRT[parityUnit$congruency=="congruent"],paired=TRUE))

```

### Response time analysis
We first analyzed response times on correct trials. To this end, we removed `r dim(dataWithErrs)[1]-dim(data)[1]` error trials (overall error rate = `r round(100*(dim(dataWithErrs)[1]-dim(data)[1])/dim(dataWithErrs)[1], 2)`%). Distributions of response times for each factorial combination of participant and experimental condition were collapsed to a single point by computing the mean RT, upon which we performed analyses of variance (ANOVA) to test the effects of our experimental manipulations. 

In order to compare the congruency effect between tasks and the different instruction conditions we first ran a two-way ANOVA for mean RTs in the overall condition with factors of task and congruency only (see Table \ref{tab:exp1table}). We found a main effect of congruency, `r summary1[["full_result"]]$congruency`. As shown in Figure \ref{fig:exp1}, responses to incongruent numbers in the overall condition were slower for both the magnitude and parity tasks. Neither the main effect of task [`r summary1[["statistic"]]$task`] nor the interaction between task and congruency [`r summary1[["statistic"]]$task_congruency`] reached significance. Hence, the compatibility effect did not differ between the magnitude and parity tasks.

We next aimed to check if the compatibility effect differed between the decade and unit conditions. To this end, we ran another ANOVA for mean RTs with independent variables of task, congruency, and the two crucial instructional conditions (decade only, unit only). There was a significant main effect of task, `r summary2[["full_result"]]$task`.  On average, the magnitude task was `r taskEffect` ms faster than the parity task.  In addition, there was a significant main effect of congruency, `r summary2[["full_result"]]$congruency`. Responses to incongruent numbers were `r congEffect` ms slower than responses to congruent numbers.  There was no main effect of instructional condition, `r summary2[["full_result"]]$condition`.

There was also an interaction between task and condition, `r summary2[["full_result"]]$task_condition`, and this interaction depended on congruency condition, as signified by the significant three-way interaction between task, instruction condition, and congruency, `r summary2[["full_result"]]$task_congruency_condition`. As shown in Figure \ref{fig:exp1}, when participants were asked to focus their attention on the decade digit and ignore the unit digits, there was no effect of congruency in the magnitude task, `r testMagDecade$statistic`.  However, there was a large effect of congruency in the parity task, `r testParityDecade$statistic`, indicating that participants suffered from intrusions from the irrelevant unit digit. By contrast, when participants were asked to pay attention to only the units digit, a large congruency effect appeared in the magnitude task, `r testMagUnit$statistic`, but the congruency effect was nonsignificant in the parity task, `r testParityUnit$statistic`.  No other terms in the ANOVA model were statistically significant.

### Error analysis

```{r errorExp1, cache=TRUE}

err1 = dataWithErrs %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(condition=="Overall") %>%
  mutate(err=1-correct) %>%
  group_by(subject, task, congruency) %>%
  summarize(errRate = 100*sum(err)/length(err))

err1.aov=aov(errRate ~ task*congruency + Error(subject/(task*congruency)), data=err1)
summaryErr1=apa_print(err1.aov, es="pes", mse=FALSE)

err2 = dataWithErrs %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(condition!="Overall") %>%
  mutate(err=1-correct) %>%
  group_by(subject, task, congruency, condition) %>%
  summarize(errRate = 100*sum(err)/length(err))

err2.aov = aov(errRate ~ task*congruency*condition + Error(subject/(task*congruency*condition)), data=err2)
summaryErr2=apa_print(err2.aov, es="pes", mse=FALSE)

```

We also analyzed how error rates differed as a function of our experimental manipulations. We first performed a two-way ANOVA for error rates in the overall condition with factors of task and congruency  only.  As with response times, there was a significant main effect of congruency, `r summaryErr1[["full_result"]]$congruency`. As can be seen in Table \ref{tab:exp1table}, error rates were larger on incongruent trials compared to congruent trials.  Neither the main effect of task [`r summaryErr1[["statistic"]]$task`] nor the interaction between task and congruency [`r summaryErr1[["statistic"]]$task_congruency`] reached significance.

We next performed an ANOVA for error rates with factors of task, congruency, and instructional condition (decade only, unit only).  There was a significant main effect of task, `r summaryErr2[["full_result"]]$task`. As can be seen in Table \ref{tab:exp1table}, participants made more errors in the parity task than they did in the magnitude task.  There was also a significant main effect of congruency, `r summaryErr2[["full_result"]]$congruency`, indicating that error rates were larger for incongruent trials than for congruent trials. Finally, there was a significant three-way interaction between task, congruency, and instructional condition, `r summaryErr2[["full_result"]]$task_congruency_condition`, driven primarily by the large error rates (and large congruency effect) for decade-only trials in the parity task. No other terms in the ANOVA model were statistically significant.

## Discussion

The pattern of unit-decade compatibility effects (which we term as *congruency* effects here) appears to depend heavily on specific task characteristics. For the magnitude task, we found a congruency effect in the overall condition but not in the decade-only condition of the magnitude task.  Since the same stimulus set was presented in both conditions, the only difference was whether we asked participants to compare the two-digit number to 55 (the overall condition) or to compare the left digit to 5 (the decade condition).  In both conditions, participants could have successfully completed the task by focusing only on the leftmost digit. However, by asking participants to compare to 55 (and thus implicitly defining the two adjacent digits as a two-digit number), we induced participants to process the irrelevant digit as well, thus resulting in the observed congruency effect. 

A different pattern emerged in the parity task.  There, we again found a congruency effect in the overall condition but not in the *unit* condition.  Again, the same stimulus set was presented in both conditions and the only difference was whether we instructed the participant to classify the parity of the two-digit number (the overall condition) or of the rightmost digit (the unit condition). In each case, the task could have been successfully completed by attending to only the rightmost digit.  However, the congruency effect was found in the overall condition but not in the unit condition.  

So far, these findings can be interpreted as follows.  As in past studies, when participants were instructed to consider the two digits as a holistic two-digit number (the overall condition), they automatically processed both digits, as was demonstrated by the observed unit-decade compatibility effect in both magnitude and parity tasks. However, such a failure of the selective attention was not always found when participants were explicitly instructed to consider the two adjacent digits in a decomposed manner (decade and unit conditions). The absence of the equivalent Stroop-like interference effect in the decade condition of the magnitude task and the unit condition of the parity task indicate that processing of the irrelevant adjacent digit is not always obligatory.  Indeed, it appears that specific task characteristics can modulate the unit-decade binding processes in two-digit number representation.

# Experiment 2

In Experiment 2, we tested the generality of our findings by replicating the basic procedure of Experiment 1, but instead of using button presses, we had participants indicate their decisions via a computer mouse click.  This allowed us additionally to track participants' hand movements during their decisions, giving a window into the evolution of the decision process [@freeman2009motions;@freemanDaleFarmer2011;@spivey2005]. As mentioned earlier, this approach has been recently used in several studies to investigate the temporal dynamics of numerical processing [see @faulkenberry2018 for a review].


```{r datagrab2, cache=TRUE}
##############
#DATA SETUP
##############

# import data
dataImport <- read_csv("~/Dropbox/papers/2018/twoDigitTaskManip/exp2data/data.csv")

dataRaw = dataImport %>%
  filter(subject!=39) # experimenter error on participant 39

# renumber subjects
subject = numeric(dim(dataRaw)[1])
for (i in 1:length(subject)){
  if (dataRaw$subject[i] < 39){
    subject[i] <- dataRaw$subject[i]
  }
  else {
    subject[i] <- dataRaw$subject[i]-1
  }
}
dataRaw$subject <- subject

correctTrials = subset(dataRaw, subset=error==0)
sub = as.factor(correctTrials$subject)
subjectParams = function(dat){
  meanRT = mean(dat$RT)
  sdRT = sd(dat$RT)
  c(meanRT, sdRT)
}

bySubject = matrix(unlist(by(correctTrials, sub, subjectParams)), ncol=2, byrow=TRUE)
    
# alternative cleaning method from Steinborn (reviewer 1)
nSub2 = length(unique(dataRaw$subject))
outlier = numeric(length(dataRaw$subject))
for (i in 1:length(dataRaw$subject)){
  meanRT = bySubject[dataRaw$subject[i] ,1]
  sdRT = bySubject[dataRaw$subject[i], 2]
  if (dataRaw$RT[i] > meanRT + 2.5*sdRT){
    outlier[i] <- 1
  }
  else {
    outlier[i] <- 0
  }
}

dataRaw$outlier = outlier

# basic info
nTrials2 = dim(dataRaw)[1]
numOutliers2 = sum(dataRaw$RT < 200 | dataRaw$outlier==1)

outlierRate2 = dataRaw %>%
  group_by(subject) %>%
  summarize(outl = 100*sum(outlier)/length(outlier))

dataWithErrs2 = dataRaw %>%
  filter(RT>200 & outlier==0) %>%
  mutate(congruency=factor(compatibility,levels=c("congruent","incongruent"))) %>%
  mutate(MT=RT-init)

data2 = dataRaw %>%
    filter(error==0 & RT > 200 & outlier==0) %>%
    mutate(congruency=factor(compatibility,levels=c("congruent","incongruent"))) %>%
    mutate(MT=RT-init)

# fix factor labels and orderings for plots
data2$instruction = factor(data2$instruction, levels = c("overall","decade","unit"), labels = c("Overall", "Decade", "Unit"))

dataWithErrs2$instruction = factor(dataWithErrs2$instruction, levels = c("overall","decade","unit"), labels = c("Overall", "Decade", "Unit"))


```
## Method

### Participants 
Sixty undergraduate students (8 male, 52 female, mean age 24.1 years, SD=5.2 years) participated in this experiment in exchange for partial course credit in their psychology courses. One participant was removed from further analysis due to an experimenter error during data collection, leaving `r nSub2` participants total. The experiment was reviewed and approved by the institutional review board at Tarleton State University.

### Apparatus 
The experiment was coded and delivered using the MouseTracker software package [@freemanAmbady2010]. All experimental trials were presented on a 20 inch iMac desktop computer with a screen resolution of 1,280 x 1,024 pixels and a refresh rate of 60 Hz. Input was captured via a Dell optical mouse connected via USB. Participants were seated in a manner such that there was a distance of approximately 60 cm between the computer display and the participants' eyes.  Participants were instructed to hold the computer mouse in a manner which was comfortable. All participants held the mouse in the right hand, positioned slightly to the right of center on the computer table. 

We ran the MouseTracker program on the iMac using a virtual Windows XP environment via Parallels. As per the recommendations of @fischerHartmann2014, we disabled the “dynamic acceleration” option and lowered the speed of the mouse movements on the screen to the second-lowest possible speed in the mouse settings dialog. This is done to prevent quick and erratic mouse movements, resulting in a smooth and more reliable record of participants’ hand movements. The resulting displacement ratio of the mouse to screen movement was 1 cm to 100 pixels.

### Stimuli and procedure

We designed computer mouse tracking versions of the same two-digit number tasks described in Experiment 1, with stimuli consisting of the 64 two-digit numbers described in Experiment 1.  Participants completed a magnitude task and a parity task according to one of three randomly assigned instructional conditions: overall ($n=19$), decade-only ($n=20$), or unit-only ($n=20$).  Participants in the overall condition were told that they would be presented with a single two-digit number.  Participants in the decade-only condition were presented with "two digits next to each other", and were instructed to pay attention to only the *left* digit.  Participants in the unit-only condition were given similar instructions, except that they were instructed to pay attention to only the *right* digit.

All participants completed 256 experimental trials, consisting of a block of 128 trials of a magnitude task and block of 128 trials of a parity task.  These task blocks were completed in counterbalanced order.  Half of the participants began with the magnitude task block, whereas the other half began with the parity task block. For participants in the overall condition, the magnitude task required participants to choose, as quickly as possible, whether the presented two-digit number was less than 55 or greater than 55.  Participants in the decade-only and unit-only conditions were instructed to choose whether the relevant single digit (left or right, respectively) was less than 5 or greater than 5.  The parity task required participants to choose whether the presented number was even or odd.  For participants in the overall condition, they were instructed to decide whether the two-digit number was even or odd, whereas participants in the decade-only and unit-only conditions were instructed to decide whether the relevant single digit (left or right, respectively) was even or odd.  

Each trial began with a blank screen presented for 500 ms, followed by a screen that displayed the appropriate response labels at the top left and right of the screen, respectively, as well as a single "Start" button at the bottom center of the screen. Each response label was presented in Arial font with point size 24.  Response labels depended on task; for the magnitude task, the response labels were "Small" and "Large", whereas for the parity task, the response labels were "Even" and "Odd". Within a single-task block (128 trials), the left-right ordering of the response labels was switched halfway through the block (i.e., after the completion of 64 trials).

Once the "Start" button was pressed, one of the 64 stimulus numerals appeared in the center of the screen, presented in Times New Roman font with point size 30. Participants then clicked on the correct response option; while doing so, the software recorded the streaming $(x, y)$ coordinates of the computer mouse approximately 63 times per second.  The left-right position of the correct response option was counterbalanced across trials within each block. 

For incorrect responses, the program displayed an “X” for 2000 ms. To ensure that trajectories reflected online processing, participants were encouraged to begin their movements as early as possible and were warned if initiated movement later than 400 ms following number pair presentation. This instruction is customarily included in mousetracking studies so that trajectories reflect the dynamics of a decision process rather than simply reflecting the kinematics of a response choice after the choice has already been made [@freeman2009motions;@spivey2005].

In total, participants completed two blocks of each of two tasks, resulting in 4 repetitions of the 64 number stimulus set.  Thus, each participant completed 256 experimental trials in a single 30 minute session.

## Results

Participants completed a total of `r nTrials2` experimental trials.  We initially discarded `r numOutliers2` outlier trials using the procedure described in Experiment 1. After this cleaning procedure, we retained `r round(100*(nTrials2-numOutliers2)/nTrials2, 1)`% of the original trials for further analysis.  Across the `r nSub2` participants, the percentage of outliers for each participant ranged between `r round(min(outlierRate2$outl),2)`% and `r round(max(outlierRate2$outl),2)`% (mean outlier rate = `r round(mean(outlierRate2$outl),2)`%).

```{r exp2table, results="asis", include=TRUE}


# format table as "magnitude" and "parity", symmetric with other plots in manuscript

# magnitude sub-table
magRT <- data2 %>%
  filter(task=="magnitude") %>%
  group_by(instruction, congruency) %>%
  summarize(meanMT = mean(MT),
            sdMT = sd(MT),
            meanInit = mean(init),
            sdInit = sd(init)
            )

magErr <- dataWithErrs2 %>%
  filter(task=="magnitude") %>%
  group_by(instruction, congruency) %>%
  summarize(errRate = 100*sum(error)/length(error))

magTable <- right_join(magRT,magErr)
names(magTable) <- c("Condition", "Congruency", "Mean", "SD", "Mean", "SD", "Error rate (%)")

# parity sub-table
parityRT <- data2 %>%
  filter(task=="parity") %>%
  group_by(instruction, congruency) %>%
  summarize(meanMT = mean(MT),
            sdMT = sd(MT),
            meanInit = mean(init),
            sdInit = sd(init)
            )

parityErr <- dataWithErrs2 %>%
  filter(task=="parity") %>%
  group_by(instruction, congruency) %>%
  summarize(errRate = 100*sum(error)/length(error))

parityTable <- right_join(parityRT,parityErr)
names(parityTable) <- c("Condition", "Congruency", "Mean", "SD", "Mean", "SD", "Error rate (%)")

apa_table(
  list('Magnitude task' = magTable, 'Parity task' = parityTable),
  caption = "Descriptive statistics for performance measures in Experiment 2",
  note = "MT = movement time; Init = initiation time.",
  col_spanners = list("MT (ms)" = c(3,4), "Init (ms)" = c(5,6)),
  align = c("l", "l", "c", "c", "c", "c", "c"),
  escape=TRUE,
  landscape=FALSE,
  small=FALSE)


```

### Time analyses

For each trial, the MouseTracker software recorded two time-based performance measures. The first of these, reaction time (RT), is defined as the total time elapsed from when the participant began a trial by clicking the START button until the target mouse click.  The second measure, initiation time (Init), is defined as the time elapsed from the initial mouse click of the START button until the onset of mouse movement. From these two measures, we calculated movement time (MT), which indexes the temporal duration of mouse movement, by subtracting RT - Init on each trial.  We present the summary statistics for these two measures (MT and Init) in Table \ref{tab:exp2table}.

```{r exp2-mt, results="hide", fig.height=4, fig.width=6, fig.cap="Mean computer mouse movement times (MTs) in Experiment 2, presented as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and task (magnitude, parity). Error bars represent within-subject 95% confidence intervals as recommended by Morey (2008).", cache=TRUE}

# plot MT figure
aggMT=aggregate(MT~subject+instruction+task+congruency,data=data2,FUN="mean") 
summary=summarySEwithin(aggMT,measurevar="MT",betweenvars=c("instruction"),withinvars=c("congruency","task"),idvar="subject")

summary %>%
  ggplot(aes(x=instruction,y=MT,shape=congruency)) +
  geom_line(aes(group=congruency,linetype=congruency))+
  geom_point(size=4)+
  geom_errorbar(width=0.1,aes(ymin=MT-ci,ymax=MT+ci))+
  labs(x="Instruction Condition",y="Mean MT (ms)")+
  facet_grid(.~task)+
  theme_classic(14)
detach(package:plyr)

# analyses
# overall condition only
overallMT = data2 %>%
  filter(instruction=="Overall") %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, task, congruency) %>%
  summarize(mMT = mean(MT)) 

overallMT.aov=aov(mMT ~ task*congruency + Error(subject/(task*congruency)), data=overallMT)
overallMTsummary=apa_print(overallMT.aov, es="pes", mse=FALSE)

# decade/unit conditions
decadeMT = data2 %>%
  filter(instruction!="Overall") %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, task, congruency,instruction) %>%
  summarize(mMT = mean(MT)) 

decadeMT.aov=aov(mMT ~ task*congruency*instruction + Error(subject/(task*congruency)), data=decadeMT)
decadeMTsummary=apa_print(decadeMT.aov, es="pes", mse=FALSE)

maxNonSig = max(sort(as.numeric(decadeMTsummary[["table"]]$`$F$`))[1:2])

# t-tests for describing 3-way interaction
magDecade = data2 %>%
  filter(instruction=="Decade" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mMT = mean(MT))
testMagDecade = apa_print(t.test(magDecade$mMT[magDecade$congruency=="incongruent"], magDecade$mMT[magDecade$congruency=="congruent"],paired=TRUE))

parityDecade = data2 %>%
  filter(instruction=="Decade" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mMT = mean(MT))
testParityDecade = apa_print(t.test(parityDecade$mMT[parityDecade$congruency=="incongruent"], parityDecade$mMT[parityDecade$congruency=="congruent"],paired=TRUE))

magUnit = data2 %>%
  filter(instruction=="Unit" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mMT = mean(MT))
testMagUnit = apa_print(t.test(magUnit$mMT[magUnit$congruency=="incongruent"], magUnit$mMT[magUnit$congruency=="congruent"],paired=TRUE))

parityUnit = data2 %>%
  filter(instruction=="Unit" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mMT = mean(MT))
testParityUnit = apa_print(t.test(parityUnit$mMT[parityUnit$congruency=="incongruent"], parityUnit$mMT[parityUnit$congruency=="congruent"],paired=TRUE))

```

#### Movement times

Similar to the approach taken in Experiment 1, we aimed to first compare the congruency effect between tasks and the different instruction conditions by first conducting a two-way ANOVA on the mean movement times (MTs) for the overall condition with factors of task and congruency only. We found a main effect of congruency, `r overallMTsummary[["full_result"]]$congruency`. As shown in Figure \ref{fig:exp2-mt}, incongruent numbers in the overall condition resulted in longer mouse movement durations for both the magnitude and parity tasks.  Neither the main effect of task [`r overallMTsummary[["full_result"]]$task`] nor the interaction between task and condition [`r overallMTsummary[["statistic"]]$task_congruency`] were significant.

Next we aimed to test whether the congruency effect differed between the decade and unit conditions. We submitted mean MTs to a three-way ANOVA with independent variables of task, congruency, and instructional condition (restricted to the decade only and unit only conditions).  Overall, the results were similar to those with RTs in Experiment 1. There was a significant main effect of task, `r decadeMTsummary[["full_result"]]$task`, indicating that MTs in the parity task were longer than those in the magnitude task. There was also a significant main effect of congruency, `r decadeMTsummary[["full_result"]]$congruency`, where MTs for incongruent numbers were longer than MTs for congruent numbers.  There was no main effect of instructional condition, `r decadeMTsummary[["full_result"]]$instruction`.

The interaction between task and instructional condition that we observed in Experiment 1 with RTs did not reach significance with MTs in Experiment 2, `r decadeMTsummary[["full_result"]]$task_instruction`. However, there was an interaction between task and congruency, `r decadeMTsummary[["full_result"]]$congruency`, and this interaction depended on instructional condition, as signified by a significant three-way interaction, `r decadeMTsummary[["full_result"]]$task_congruency_instruction`. As shown in Figure \ref{fig:exp2-mt}, when participants were asked to focus only on the decade digit and ignore the unit digit, there was no effect of congruency on MTs in the magnitude task, `r testMagDecade$statistic`.  However, we did observe a large congruency effect in the parity task, `r testParityDecade$statistic`, indicating a failure of selective attention on the parity task.  In contrast to Experiment 1, when we asked participants to pay attention to only the units digit, a large congruency effect appeared in both the magnitude task, `r testMagUnit$statistic`, and the parity task, `r testParityUnit$statistic`.  No other terms in the ANOVA model were statistically significant.


```{r exp2-init, results="hide", fig.height=4, fig.width=6, fig.cap="Mean mouse initiation times as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and task (magnitude, parity). Error bars represent within-subject 95% confidence intervals as recommended by Morey (2008).", cache=TRUE}

# plot mean Inits

aggInit=aggregate(init~subject+instruction+task+congruency,data=data2,FUN="mean")
summary=summarySEwithin(aggInit,measurevar="init",betweenvars=c("instruction"),withinvars=c("congruency","task"),idvar="subject")

summary %>%
  ggplot(aes(x=instruction,y=init,shape=congruency)) +
  geom_line(aes(group=congruency,linetype=congruency))+
  geom_point(size=4)+
  geom_errorbar(width=0.1,aes(ymin=init-ci,ymax=init+ci))+
  labs(x="Instruction Condition",y="Mean Initiation Time (ms)")+
  facet_grid(.~task)+
  theme_classic(14)
detach(package:plyr)

# analyses

init = data2 %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, task, congruency, instruction) %>%
  summarize(mInit = mean(init)) 

init.aov=aov(mInit ~ task*congruency*instruction + Error(subject/(task*congruency)), data=init)
initSummary=apa_print(init.aov, es="pes", mse=FALSE)

maxNonSig = max(sort(as.numeric(initSummary[["table"]]$`$F$`))[1:5])


initByInstruction = data2 %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, instruction) %>%
  summarize(mInit = mean(init)) 

test1 = apa_print(t.test(initByInstruction$mInit[initByInstruction$instruction=="Overall"], initByInstruction$mInit[initByInstruction$instruction=="Decade"], var.equal=TRUE))

test2 = apa_print(t.test(initByInstruction$mInit[initByInstruction$instruction=="Overall"], initByInstruction$mInit[initByInstruction$instruction=="Unit"], var.equal=TRUE))

test3 = apa_print(t.test(initByInstruction$mInit[initByInstruction$instruction=="Decade"], initByInstruction$mInit[initByInstruction$instruction=="Unit"], var.equal=TRUE))

```

#### Initiation times

Mouse initiation times were submitted to a three-way ANOVA with factors of congruency, task, and instruction condition.  There was a significant main effect of congruency, `r initSummary[["full_result"]]$congruency`, with faster initiation times on incongruent trials.  The effect of instructional condition on initiation time was not statistically significant, `r initSummary[["statistic"]]$instruction`, though it is worth noting that Figure \ref{fig:exp2-init} appears to show that initiation times were longer for the overall condition, particularly for the magnitude task. No other terms in the ANOVA model were significant.

### Error analysis

```{r errorExp2, cache=TRUE}

err2 = dataWithErrs2 %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(instruction=="Overall") %>%
  group_by(subject, task, congruency) %>%
  summarize(errRate = 100*sum(error)/length(error))

err2.aov=aov(errRate ~ task*congruency + Error(subject/(task*congruency)), data=err2)
summaryErr2=apa_print(err2.aov, es="pes", mse=FALSE)

err3 = dataWithErrs2 %>%
  mutate(task=as.factor(task), congruency=as.factor(congruency), subject=as.factor(subject)) %>%
  filter(instruction!="Overall") %>%
  group_by(subject, task, congruency, instruction) %>%
  summarize(errRate = 100*sum(error)/length(error))

err3.aov = aov(errRate ~ task*congruency*instruction + Error(subject/(task*congruency)), data=err3)
summaryErr3 = apa_print(err3.aov, es="pes", mse=FALSE)

```

Errors in Experiment 2 were relatively rare, with only `r dim(dataWithErrs2)[1]-dim(data2)[1]` errors across all participants (error rate = `r 100*(dim(dataWithErrs2)[1]-dim(data2)[1])/dim(dataWithErrs2)[1]`%). As we did in Experiment 1, we submitted error rates in the overall condition to a two-way ANOVA with factors of task and congruency only.  There was a significant main effect of congruency, `r summaryErr2[["full_result"]]$congruency`. As can be seen in Table \ref{tab:exp2table}, error rates were larger on incongruent trials compared to congruent trials.  Neither the main effect of task nor the interaction between task and congruency reached significance.

We also performed an ANOVA for error rates with factors of task, congruency, and instructional condition (decade only, unit only).  There was a significant main effect of congruency, `r summaryErr3[["full_result"]]$congruency`, indicating again that error rates were larger for incongruent trials than for congruent trials. No other terms in the ANOVA model were statistically significant.

### Trajectory analyses

Computer mouse trajectories were measured by recording the streaming $x, y$ - coordinates of the computer mouse during each trial.  The raw trajectories were then pre-processed and normalized in MouseTracker [@freemanAmbady2010] to allow for ease of analysis across different experimental conditions. Specifically, all trajectories were rescaled onto a standard coordinate space of [-1,1] x [0,1.5] and normalized via linear interpolation to consist of exactly 101 timesteps. This step allowed for direct comparison of trajectory curvatures, indexed via area under the curve (AUC) and maximum deviation (MD) from a direct trajectory (i.e., a straight line from start to target). Note that since MouseTracker computes AUC for each trial by summing the areas of 100 trapezoids that comprise the area between the trajectory path and the ideal straight line between the start box and the response box, it is essential that the trajectories each be comprised of the same *number* of timesteps.

```{r magnitudeTrajectories, results="hide", fig.height=4, fig.width=6, fig.cap="Average normalized mouse trajectories for the magnitude task, presented as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and response side (left, right).", cache=TRUE}

data2 = as.data.frame(data2)
# plot hand trajectories for magnitude task
dataLeftOverallCongruent<-subset(data2,response==1 & instruction=="Overall" & congruency=="congruent" & task=="magnitude")
dataLeftOverallIncongruent<-subset(data2,response==1 & instruction=="Overall" & congruency=="incongruent" & task=="magnitude")
dataRightOverallCongruent<-subset(data2,response==2 & instruction=="Overall" & congruency=="congruent" & task=="magnitude")
dataRightOverallIncongruent<-subset(data2,response==2 & instruction=="Overall" & congruency=="incongruent" & task=="magnitude")

dataLeftDecadeCongruent<-subset(data2,response==1 & instruction=="Decade" & congruency=="congruent" & task=="magnitude")
dataLeftDecadeIncongruent<-subset(data2,response==1 & instruction=="Decade" & congruency=="incongruent" & task=="magnitude")
dataRightDecadeCongruent<-subset(data2,response==2 & instruction=="Decade" & congruency=="congruent" & task=="magnitude")
dataRightDecadeIncongruent<-subset(data2,response==2 & instruction=="Decade" & congruency=="incongruent" & task=="magnitude")

dataLeftUnitCongruent<-subset(data2,response==1 & instruction=="Unit" & congruency=="congruent" & task=="magnitude")
dataLeftUnitIncongruent<-subset(data2,response==1 & instruction=="Unit" & congruency=="incongruent" & task=="magnitude")
dataRightUnitCongruent<-subset(data2,response==2 & instruction=="Unit" & congruency=="congruent" & task=="magnitude")
dataRightUnitIncongruent<-subset(data2,response==2 & instruction=="Unit" & congruency=="incongruent" & task=="magnitude")

xCoords=rep(0,1212)
yCoords=rep(0,1212)
response=rep(0,1212)
instruction=rep(0,1212)
congruency=rep(0,1212)

# x-coordinates start in column 22

for (i in 1:101){
  xCoords[i]=mean(dataLeftOverallCongruent[,i+21])
  yCoords[i]=mean(dataLeftOverallCongruent[,i+122])
  response[i]="left"
  instruction[i]="Overall"
  congruency[i]="congruent"

  xCoords[i+101]=mean(dataLeftOverallIncongruent[,i+21])
  yCoords[i+101]=mean(dataLeftOverallIncongruent[,i+122])
  response[i+101]="left"
  instruction[i+101]="Overall"
  congruency[i+101]="incongruent"
  
  xCoords[i+202]=mean(dataRightOverallCongruent[,i+21])
  yCoords[i+202]=mean(dataRightOverallCongruent[,i+122])
  response[i+202]="right"
  instruction[i+202]="Overall"
  congruency[i+202]="congruent"
  
  xCoords[i+303]=mean(dataRightOverallIncongruent[,i+21])
  yCoords[i+303]=mean(dataRightOverallIncongruent[,i+122])
  response[i+303]="right"
  instruction[i+303]="Overall"
  congruency[i+303]="incongruent"
  
  xCoords[i+404]=mean(dataLeftDecadeCongruent[,i+21])
  yCoords[i+404]=mean(dataLeftDecadeCongruent[,i+122])
  response[i+404]="left"
  instruction[i+404]="Decade"
  congruency[i+404]="congruent"
  
  xCoords[i+505]=mean(dataLeftDecadeIncongruent[,i+21])
  yCoords[i+505]=mean(dataLeftDecadeIncongruent[,i+122])
  response[i+505]="left"
  instruction[i+505]="Decade"
  congruency[i+505]="incongruent"
  
  xCoords[i+606]=mean(dataRightDecadeCongruent[,i+21])
  yCoords[i+606]=mean(dataRightDecadeCongruent[,i+122])
  response[i+606]="right"
  instruction[i+606]="Decade"
  congruency[i+606]="congruent"
  
  xCoords[i+707]=mean(dataRightDecadeIncongruent[,i+21])
  yCoords[i+707]=mean(dataRightDecadeIncongruent[,i+122])
  response[i+707]="right"
  instruction[i+707]="Decade"
  congruency[i+707]="incongruent"
 
  xCoords[i+808]=mean(dataLeftUnitCongruent[,i+21])
  yCoords[i+808]=mean(dataLeftUnitCongruent[,i+122])
  response[i+808]="left"
  instruction[i+808]="Unit"
  congruency[i+808]="congruent"
  
  xCoords[i+909]=mean(dataLeftUnitIncongruent[,i+21])
  yCoords[i+909]=mean(dataLeftUnitIncongruent[,i+122])
  response[i+909]="left"
  instruction[i+909]="Unit"
  congruency[i+909]="incongruent"
  
  xCoords[i+1010]=mean(dataRightUnitCongruent[,i+21])
  yCoords[i+1010]=mean(dataRightUnitCongruent[,i+122])
  response[i+1010]="right"
  instruction[i+1010]="Unit"
  congruency[i+1010]="congruent"
  
  xCoords[i+1111]=mean(dataRightUnitIncongruent[,i+21])
  yCoords[i+1111]=mean(dataRightUnitIncongruent[,i+122])
  response[i+1111]="right"
  instruction[i+1111]="Unit"
  congruency[i+1111]="incongruent"
}

trajectoryData=data.frame(xCoords,yCoords,response,instruction,congruency)
trajectoryData$instruction = factor(trajectoryData$instruction, levels=c("Overall", "Decade", "Unit"))

trajectoryData %>%
  ggplot(aes(x=xCoords,y=yCoords,group=congruency)) +
  xlim(-1,1) + 
  ylim(0,1.5) + 
  geom_path(aes(linetype=congruency),size=0.8) + 
  labs(x="x-coordinates",y="y-coordinates") + 
  facet_grid(instruction~response) + 
  theme_classic(14)

```

```{r parityTrajectories, results="hide", fig.height=4, fig.width=6, fig.cap="Average normalized mouse trajectories for the parity task, presented as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and response side (left, right).", cache=TRUE}

data2 = as.data.frame(data2)
# plot hand trajectories for magnitude task
dataLeftOverallCongruent<-subset(data2,response==1 & instruction=="Overall" & congruency=="congruent" & task=="parity")
dataLeftOverallIncongruent<-subset(data2,response==1 & instruction=="Overall" & congruency=="incongruent" & task=="parity")
dataRightOverallCongruent<-subset(data2,response==2 & instruction=="Overall" & congruency=="congruent" & task=="parity")
dataRightOverallIncongruent<-subset(data2,response==2 & instruction=="Overall" & congruency=="incongruent" & task=="parity")

dataLeftDecadeCongruent<-subset(data2,response==1 & instruction=="Decade" & congruency=="congruent" & task=="parity")
dataLeftDecadeIncongruent<-subset(data2,response==1 & instruction=="Decade" & congruency=="incongruent" & task=="parity")
dataRightDecadeCongruent<-subset(data2,response==2 & instruction=="Decade" & congruency=="congruent" & task=="parity")
dataRightDecadeIncongruent<-subset(data2,response==2 & instruction=="Decade" & congruency=="incongruent" & task=="parity")

dataLeftUnitCongruent<-subset(data2,response==1 & instruction=="Unit" & congruency=="congruent" & task=="parity")
dataLeftUnitIncongruent<-subset(data2,response==1 & instruction=="Unit" & congruency=="incongruent" & task=="parity")
dataRightUnitCongruent<-subset(data2,response==2 & instruction=="Unit" & congruency=="congruent" & task=="parity")
dataRightUnitIncongruent<-subset(data2,response==2 & instruction=="Unit" & congruency=="incongruent" & task=="parity")

xCoords=rep(0,1212)
yCoords=rep(0,1212)
response=rep(0,1212)
instruction=rep(0,1212)
congruency=rep(0,1212)

# x-coordinates start in column 22

for (i in 1:101){
  xCoords[i]=mean(dataLeftOverallCongruent[,i+21])
  yCoords[i]=mean(dataLeftOverallCongruent[,i+122])
  response[i]="left"
  instruction[i]="Overall"
  congruency[i]="congruent"

  xCoords[i+101]=mean(dataLeftOverallIncongruent[,i+21])
  yCoords[i+101]=mean(dataLeftOverallIncongruent[,i+122])
  response[i+101]="left"
  instruction[i+101]="Overall"
  congruency[i+101]="incongruent"
  
  xCoords[i+202]=mean(dataRightOverallCongruent[,i+21])
  yCoords[i+202]=mean(dataRightOverallCongruent[,i+122])
  response[i+202]="right"
  instruction[i+202]="Overall"
  congruency[i+202]="congruent"
  
  xCoords[i+303]=mean(dataRightOverallIncongruent[,i+21])
  yCoords[i+303]=mean(dataRightOverallIncongruent[,i+122])
  response[i+303]="right"
  instruction[i+303]="Overall"
  congruency[i+303]="incongruent"
  
  xCoords[i+404]=mean(dataLeftDecadeCongruent[,i+21])
  yCoords[i+404]=mean(dataLeftDecadeCongruent[,i+122])
  response[i+404]="left"
  instruction[i+404]="Decade"
  congruency[i+404]="congruent"
  
  xCoords[i+505]=mean(dataLeftDecadeIncongruent[,i+21])
  yCoords[i+505]=mean(dataLeftDecadeIncongruent[,i+122])
  response[i+505]="left"
  instruction[i+505]="Decade"
  congruency[i+505]="incongruent"
  
  xCoords[i+606]=mean(dataRightDecadeCongruent[,i+21])
  yCoords[i+606]=mean(dataRightDecadeCongruent[,i+122])
  response[i+606]="right"
  instruction[i+606]="Decade"
  congruency[i+606]="congruent"
  
  xCoords[i+707]=mean(dataRightDecadeIncongruent[,i+21])
  yCoords[i+707]=mean(dataRightDecadeIncongruent[,i+122])
  response[i+707]="right"
  instruction[i+707]="Decade"
  congruency[i+707]="incongruent"
 
  xCoords[i+808]=mean(dataLeftUnitCongruent[,i+21])
  yCoords[i+808]=mean(dataLeftUnitCongruent[,i+122])
  response[i+808]="left"
  instruction[i+808]="Unit"
  congruency[i+808]="congruent"
  
  xCoords[i+909]=mean(dataLeftUnitIncongruent[,i+21])
  yCoords[i+909]=mean(dataLeftUnitIncongruent[,i+122])
  response[i+909]="left"
  instruction[i+909]="Unit"
  congruency[i+909]="incongruent"
  
  xCoords[i+1010]=mean(dataRightUnitCongruent[,i+21])
  yCoords[i+1010]=mean(dataRightUnitCongruent[,i+122])
  response[i+1010]="right"
  instruction[i+1010]="Unit"
  congruency[i+1010]="congruent"
  
  xCoords[i+1111]=mean(dataRightUnitIncongruent[,i+21])
  yCoords[i+1111]=mean(dataRightUnitIncongruent[,i+122])
  response[i+1111]="right"
  instruction[i+1111]="Unit"
  congruency[i+1111]="incongruent"
}

trajectoryData=data.frame(xCoords,yCoords,response,instruction,congruency)
trajectoryData$instruction = factor(trajectoryData$instruction, levels=c("Overall", "Decade", "Unit"))

trajectoryData %>%
  ggplot(aes(x=xCoords,y=yCoords,group=congruency)) +
  xlim(-1,1) + 
  ylim(0,1.5) + 
  geom_path(aes(linetype=congruency),size=0.8) + 
  labs(x="x-coordinates",y="y-coordinates") + 
  facet_grid(instruction~response) + 
  theme_classic(14)

```

```{r exp2table-traj, results="asis", include=TRUE}


# format table as "magnitude" and "parity", symmetric with other plots in manuscript

# magnitude sub-table
magTable <- data2 %>%
  filter(task=="magnitude") %>%
  group_by(instruction, congruency) %>%
  summarize(meanAUC = mean(AUC),
            sdAUC = sd(AUC),
            meanMD = mean(MD),
            sdMD = sd(MD)
            )

names(magTable) <- c("Condition", "Congruency", "Mean", "SD", "Mean", "SD")

# parity sub-table
parityTable <- data2 %>%
  filter(task=="parity") %>%
  group_by(instruction, congruency) %>%
  summarize(meanAUC = mean(AUC),
            sdAUC = sd(AUC),
            meanMD = mean(MD),
            sdMD = sd(MD)
            )

names(parityTable) <- c("Condition", "Congruency", "Mean", "SD", "Mean", "SD")

apa_table(
  list('Magnitude task' = magTable, 'Parity task' = parityTable),
  caption = "Descriptive statistics for trajectory measures in Experiment 2",
  note = "AUC = area under curve; MD = maximum deviation.",
  col_spanners = list("AUC" = c(3,4), "MD" = c(5,6)),
  align = c("l", "l", "c", "c", "c", "c"),
  escape=TRUE,
  landscape=FALSE,
  small=FALSE)


```

To see how the magnitude and parity decisions unfolded over time, we plotted average mouse trajectories as a function of congruency and instruction condition.  Trajectories for the magnitude task are depicted in Figure \ref{fig:magnitudeTrajectories}, and trajectories for the parity task are depicted in Figure \ref{fig:parityTrajectories}.

```{r exp2-auc, results="hide", fig.height=4, fig.width=6, fig.cap="Mean area-under-the-curve (AUC) values for Experiment 2, presented as a function of congruency (congruent, incongruent), instruction condition (overall, decade only, unit only), and task (magnitude, parity). Error bars represent within-subject 95% confidence intervals as recommended by Morey (2008).", cache=TRUE}

# plot MT figure
aggAUC=aggregate(AUC~subject+instruction+task+congruency,data=data2,FUN="mean") 
summary=summarySEwithin(aggAUC,measurevar="AUC",betweenvars=c("instruction"),withinvars=c("congruency","task"),idvar="subject")

summary %>%
  ggplot(aes(x=instruction,y=AUC,shape=congruency)) +
  geom_line(aes(group=congruency,linetype=congruency))+
  geom_point(size=4)+
  geom_errorbar(width=0.1,aes(ymin=AUC-ci,ymax=AUC+ci))+
  labs(x="Instruction Condition",y="Mean AUC")+
  facet_grid(.~task)+
  theme_classic(14)
detach(package:plyr)

# analyses
# overall condition only
overallAUC = data2 %>%
  filter(instruction=="Overall") %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, task, congruency) %>%
  summarize(mAUC = mean(AUC)) 

overallAUC.aov=aov(mAUC ~ task*congruency + Error(subject/(task*congruency)), data=overallAUC)
overallAUCsummary=apa_print(overallAUC.aov, es="pes", mse=FALSE)

# decade/unit conditions
decadeAUC = data2 %>%
  filter(instruction!="Overall") %>%
  mutate(task=as.factor(task), subject=as.factor(subject)) %>%
  group_by(subject, task, congruency,instruction) %>%
  summarize(mAUC = mean(AUC)) 

decadeAUC.aov=aov(mAUC ~ task*congruency*instruction + Error(subject/(task*congruency)), data=decadeAUC)
decadeAUCsummary=apa_print(decadeAUC.aov, es="pes", mse=FALSE)

maxNonSig = max(sort(as.numeric(initSummary[["table"]]$`$F$`))[1:2])

# t-tests for describing 3-way interaction
magDecade = data2 %>%
  filter(instruction=="Decade" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mAUC = mean(AUC))
testMagDecade = apa_print(t.test(magDecade$mAUC[magDecade$congruency=="incongruent"], magDecade$mAUC[magDecade$congruency=="congruent"],paired=TRUE))

parityDecade = data2 %>%
  filter(instruction=="Decade" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mAUC = mean(AUC))
testParityDecade = apa_print(t.test(parityDecade$mAUC[parityDecade$congruency=="incongruent"], parityDecade$mAUC[parityDecade$congruency=="congruent"],paired=TRUE))

magUnit = data2 %>%
  filter(instruction=="Unit" & task=="magnitude") %>%
  group_by(subject,congruency) %>%
  summarize(mAUC = mean(AUC))
testMagUnit = apa_print(t.test(magUnit$mAUC[magUnit$congruency=="incongruent"], magUnit$mAUC[magUnit$congruency=="congruent"],paired=TRUE))

parityUnit = data2 %>%
  filter(instruction=="Unit" & task=="parity") %>%
  group_by(subject,congruency) %>%
  summarize(mAUC = mean(AUC))
testParityUnit = apa_print(t.test(parityUnit$mAUC[parityUnit$congruency=="incongruent"], parityUnit$mAUC[parityUnit$congruency=="congruent"],paired=TRUE))

```

As with our other dependent measures, we tested how the congruency effect (as manifested in trajectory curvatures) differed between the three instructional conditions. Recall that curvature is numerically indexed by two measures: AUC (area under the curve) and MD (maximum deviation). Summary statistics for AUC and MD can be seen in Table \ref{tab:exp2table-traj}.  Since the pattern of results did not differ between AUC and MD, we report only the results for AUC. We first submitted mean AUCs for participants in the overall condition to a two-way ANOVA with independent variables of task and congruency.  As can be seen in Figure \ref{fig:exp2-auc}, we observed a significant main effect of task, `r overallAUCsummary[["full_result"]]$task`, indicating that curvatures in the parity task were greater than those in the magnitude task. There was also a significant main effect of congruency, `r overallAUCsummary[["full_result"]]$congruency`, where curvatures for incongruent numbers were greater than curvatures for congruent numbers.  The interaction between task and congruency was not statistically significant, `r overallAUCsummary[["full_result"]]$task_congruency`.

Next we submitted mean AUCs in the decade-only and unit-only conditions to a three-way ANOVA with independent variables of task, congruency, and instructional condition. There was a significant main effect of instruction, `r decadeAUCsummary[["full_result"]]$instruction`, indicating that curvatures in the unit-only condition were greater than those in the decade-only condition. There was also a significant main effect of congruency, `r decadeAUCsummary[["full_result"]]$congruency`, where curvatures for incongruent numbers were greater than curvatures for congruent numbers.  There was also a main effect of task, `r decadeAUCsummary[["full_result"]]$task`, reflecting greater curvatures for the parity task than for the magnitude task.

Further, we observed a significant interaction between congruency and instruction, `r decadeAUCsummary[["full_result"]]$congruency_instruction`, and this interaction depended on task, as signified by a significant three-way interaction, `r decadeAUCsummary[["full_result"]]$task_congruency_instruction`.  As shown in Figure \ref{fig:exp2-auc}, participants who were asked to focus only on the decade digit and ignore the unit digit exhibited no effect of congruency on AUCs in the magnitude task, `r testMagDecade$statistic`.  However, as we observed with RTs (in Experiment 1) and MTs (in Experiment 2), we did observe a large congruency effect in the parity task, `r testParityDecade$statistic`.  Finally, when participants were asked to pay attention to only the units digit, a large congruency effect appeared in both the magnitude task, `r testMagUnit$statistic`, and the parity task, `r testParityUnit$statistic`.  No other terms in the ANOVA model were statistically significant.

## Discussion

Experiment 2 provides further evidence that varying task instructions may cause participants to modulate their focus of attention to the surrounding digits, depending on their respective relevance for accurate task completion. As in Experiment 1, the overall condition yielded similar pattern of congruency effects across the two tasks (magnitude and parity).  But, different patterns occurred when participants were asked to focus only on one digit. For example, when the task required magnitude comparison on *unit* targets, participants' mouse trajectories were initially biased towards the irrelevant digit in the decade position, resulting in relatively large reach curvatures and longer MTs on incongruent compared to congruent trials. In contrast, when the task required magnitude comparison on *decade* targets, the absence of curvature and MT differences between congruent and incongruent trials implies that the reaching movements were immune to interference from the irrelevant digit in the unit position. On the other hand, when the task required parity judgment, we observed interference effects (longer MTs, greater AUCs) with varying strengths from incongruent primes in both positions, suggesting that irrelevant digits in both decade and unit positions can bias participants' spatial attention towards competing response options. 

It is perhaps interesting that movement initiation was relatively immune to our experimental manipulations.  However, we did find a small effect of congruency on initiation times, as well as a trend toward longer initiation times over the course of overall condition (where participants focus on the whole digit) as compared to decomposed conditions (where participants focus on a single component). The differences in initiation times suggest that the speed with which a response was initiated may have been influenced by perceptual complexity of the stimulus number. It may be the case that target digits with longer length (i.e., two digits versus a single digit) might increase perceptual load, which in turn increases the time for response initiation. This interpretation of the data would be consistent with previous research indicating that initiation time and reach curvature originate from two dissociable processes [e.g., @erb2016;@ruitenberg2016;@strauss2015]. Specifically, it is thought that initiation times might be driven by an early action planning stage that is tied to cue-related encoding processes, whereas MT and AUC might reflect mainly top-down (controlled) processes which provide for revision and correction of the decision during the movement execution phase [e.g., @faulkenberryShaki2016;@sobel2016;@sobel2017]. All together, our analysis of movement dynamics echoes and extends the results obtained using the traditional keypress paradigm in Experiment 1. Possible implications of these findings for understanding the time-course of two-digit number representation are addressed in the General Discussion.

# General Discussion

The aim of the present study was to investigate unit-decade bindings in two-digit numbers and examine how they are created as function of task instruction. To this end, we asked participants to perform parity or magnitude tasks while paying attention to a specific digit (either the decade digit or the unit digit). Critically, we manipulated congruency of the irrelevant distractor digit with respect to the specific task. In the two experiments, participants in the magnitude task experienced a large degree of interference from an incongruent prime in the decade digit when asked to pay attention to only the unit digit.  On the other hand, when participants were asked to pay attention to only the decade digit, no such interference from the incongruent prime in the unit position was observed. A different pattern emerged in the parity task, where interference effects from incongruent primes were observed in both instructional conditions.  Thus, changes in task instruction are sufficient to divert attentional resources to a specific position within a two-digit number, indicating that (1) people form decomposed representations of two-digit numbers, and (2) unit-decade binding is not always obligatory.  This finding is consistent with previous research indicating that the amount of attentional weight given to the processing of decades or units can be regulated by manipulating task characteristics [e.g., @castronovo2011;@macizo2011;@reynvoet2011]. Moreover, such evidence constrains the syntactic structure model of two-digit numbers which predicts that compatibility should be mainly affected by numbers' decade digits but not by numbers' unit digits [@ganorStern2007]. All together, these results, combined with previous studies, suggest that two-digit number representations are adaptive to manipulations of experimental context.

The asymmetry of the unit-decade compatibility effects observed in the present study can be interpreted as a consequence of the dynamic reorganization of attentional weights that are associated with the different roles that decades and units play in a given task. For example, when performing magnitude comparison in two-digit numbers, task requirements dictate that participants should focus a large portion of their attention on the decade digit, as the decade must necessarily be processed in order to perform the comparison task. This increased attentional weight toward the decade digit apparently persists when people are asked to focus only on the unit digit.  However, when people are asked to focus only on the decade digit, no such interference occurs. It is important to stress that the present study goes beyond previous experiments that examined unit-decade compatibility because here we show that compatibility between two-digits can emerge even when participants are asked to pay attention to a single component of a two-digit number. 

Another goal of the present study was to establish the point in the stimulus processing stream at which the unit-decade conflict first emerges and how it is resolved over time.  We did so by analyzing the early and late kinematic parameters afforded by computer mouse tracking. In this analyses, we detected two distinctive influences with time-varying patterns on response decisions. The first notable influence appeared in the earliest stages of movement with initiation times.  Specifically, the speed with which participants initiated their hand movements was influenced by complexity of the string of digits (holistic vs. decomposed), but less so by congruency condition. In contrast to this, the congruency effect from incongruent primes was evident relatively late in movement, as trajectories showed more attraction toward the incorrect response option on incongruent than on congruent trials. 

We next discuss the implications of these findings for understanding how two-digit number processing unfolds over time. One possible explanation is that the initiation time measure in our study might reflect an early processing stage at which the identity of the digits is already coded but not yet assigned to the corresponding unit and decade positions within the number. Only at a later phase of movement execution, as more information accumulates, participants start associating digit identities with a specific position within the number, resulting in increased attraction of response trajectories toward the incorrect response on incongruent trials [see @dotan2013 for a similar argument with number-to-position task]. This interpretation of the data fits well with a dynamic view of decision-making which describes decision process as a continuous competition between response alternatives in which initial choice can be revised at any time during the movement as soon as new information becomes available [@spivey2005]. This explanation would also be consistent with earlier findings favoring a distinction between effects of action planning and online control [@erb2016;@ruitenberg2016;@strauss2015].

Moreover, our findings may provide an empirical validation for recent conceptual distinction introduced by @nuerk2014chapter assuming that the place-value system for multi-digit numbers is subserved by different steps. A first step, place-identification, refers to the earliest stages of perceptual encoding at which relevant symbols (Arabic digits) are differentiated based on their positional identity (i.e., units, tens, etc), without activating their corresponding semantic values. At the subsequent step, place-value activation, the semantic values associated with those digit positions become activated, leading to the empirically observed compatibility effect. A third step, place-value computation, is recruited in particular situations when place-value properties are required to solve a multi-digit arithmetic task, and thus is not relevant for the present discussion. The difference found between early and late measures of mouse movement is consistent with the above suggestion that early visual identification processing – at least in part – precedes the position coding processes that takes place later and causes the congruency effect. One possible reason for this is that place-value coding relies on availability of binding and maintenance processes in working memory, which takes more time to retrieve, thereby, cascading it to later stages of processing. Another possibility is that competition between response alternatives takes place only when overt response starts, thus becoming visible only during a response-related stage of processing. Further research is needed to disentangle between these alternative possibilities.

Next, we provide a theoretical framework to explain how task instructions may modulate unit-decade binding in two-digit number comparison. For this purpose, we use a variant of dual-route model that has been shown to account for a number of key findings in the number domain, such as the SNARC effect and the size congruity effect [@gevers2006;@santens2011]. According to this model, congruity effects originate from parallel activation of two independently operating routes that interact at the decision level. The first (unconditional) route automatically processes task-irrelevant stimulus information and primes the associated response, while the second (conditional) route codes for task-relevant stimulus information and is activated intentionally on basis of task instructions. Changing the instructions (task-related route) leads to a change in the pattern of weights that determine which stimulus features are bound to response features. The functional differences between the mapping of relevant and irrelevant stimulus features to responses (on the basis of instructions) is sufficient to create different types of congruency. To see this, suppose that a task entails a magnitude comparison of the unit digit only. Participants are then presented with stimulus "78", for which (in this specific task) decade places are irrelevant. In this case, both routes (conditional and unconditional) activate the same response, since with a compatible number like "78", both digits are larger than referent value of 5.  The result is that a response is executed relatively quickly.  However, when a new task instruction is given emphasizing parity judgment, pre-existing links between stimulus features and responses are re-bound to form a new meaning. More specifically, now both routes converge on opposing responses, as now "78" is an incompatible number, since the two digits have opposite parity status.  The result here is that RTs are increased.  The main point is that the proposed model accounts for two key characteristics responsible for the compatibility effect in our study: namely, the modulatory effects of instructions and its response-related origin. However, to account for the observed asymmetry in our data, the model needs to make additional assumptions with respect to how strongly the dominant stimulus values (decades or units) are associated with response options (magnitude or parity) such that the gradient of connections is increasing to one of the response labels but decreasing to the other.

Finally, we discuss some possible limitations of our study. First, despite the efforts to achieve comparability by applying the same stimulus sets and mostly equivalent procedural details to parity and magnitude tasks, the present study does not rule out a potential contribution of mechanistic differences between these two tasks. At least in the context of the SNARC effect, previous researchers have pointed out that parity and magnitude judgment are not equivalent with respect to working memory resources [@herrera2008;@vanDijck2009]. Additionally, it could be argued that in situations where subjects shift between tasks, the history of previous experience with, for example, a magnitude comparison task, may remain active and later interfere with a new task goal of judging parity. Under these circumstances, it is unclear whether the observed pattern of interference effects are driven by activation of the current or previously seen context when switching between two tasks [@kiesel2010;@vandierendonck2010]. To counteract for such task-transition effects, we further validated our methodological approach in Experiment 2 by manipulating the instruction assignment for the magnitude comparison and parity-judgment tasks *between* rather than within subjects. This procedure helped to ensure that distractor category conflicted with the target category on only one dimension of task context (i.e., whether the task was parity or magnitude). The results of Experiment 2 were largely similar to those of Experiment 1, providing further evidence that components of a two-digit number become bound due to instructional context rather than via an *a priori* representational binding.

Note also that in our experiment we varied task instruction by specifically asking participants to process the presented two-digit numbers in a task-defined manner. This instructional manipulation is *imperative*; participants must attend to specific digits in order to successfully complete the task without error. As such, our experiment is not exactly the same as an *emphasis* manipulation, where the attentional weight of the units or decades is selectively varied. In our discussions throughout this manuscript, we have addressed the possibility that our observed performance patterns are due to the changing of attentional weights; at present, this is purely speculative. A stronger case could be made in future work if one could study the role of varying attentional weights directly, either by manipulating selectivity of attention [@logan1996] or intensity of attention [@steinborn2016].  

To summarize, our study demonstrates the impact of task instructions for unit-decade binding in two-digit number representation.  We have demonstrated that processing both components of two-digit numbers is obligatory only if participants have been asked to consider the two-digit number as an integral mathematical whole. However, if participants have been explicitly asked to pay attention to one digit only, their ability to ignore the irrelevant adjacent digit depends on the task requirements.  This asymmetrical pattern in selective attention indicates the deep involvement of top-down processes in two-digit number representation.

# Compliance with ethical standards

#### Conflict of interest 
The authors declare that they have no conflict of interest.

#### Ethical approval
All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committees and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards.

#### Informed consent
Informed consent was obtained from all individual participants included in the study.

\newpage

# References
```{r create_r-references}
r_refs(file = "references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
